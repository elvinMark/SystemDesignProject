{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from natto import MeCab\n",
    "import pandas as pd\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  #DEBUG を INFOに変えるとlogging.debugが出力されなくなる\n",
    "\n",
    "\n",
    "#授業の使いまわし\n",
    "def parse2df(text,sysdic=\"/usr/local/lib/mecab/dic/naist-jdic\"):\n",
    "    \"\"\"文毎に形態素解析を行い、結果をdataframeに格納して返す\n",
    "    Args:\n",
    "      text:形態素解析対象のテキスト\n",
    "    Returns:\n",
    "      形態素解析結果を格納したdataframe\n",
    "      カラムは['文番号','表層', '品詞1','品詞2','品詞3','品詞4','原型','posID']\n",
    "    \"\"\"\n",
    "    #結果格納用の空のDataFrame\n",
    "    df = pd.DataFrame(index=[], columns=['文番号','表層', '品詞1','品詞2','品詞3','品詞4','原型','posID'])\n",
    "    \n",
    "    text = text.split(\"\\n\") #改行で分割して配列にする\n",
    "    while '' in text: #空行は削除\n",
    "        text.remove('')\n",
    "    \n",
    "    parser = MeCab(\"-d \"+sysdic)\n",
    "\n",
    "    for index,sentence in enumerate(text): \n",
    "        logging.debug(sentence)\n",
    "        nodes = parser.parse(sentence,as_nodes=True)\n",
    "        for node in nodes:\n",
    "            if not node.is_eos():\n",
    "                #品詞情報を分割\n",
    "                feature = node.feature.split(',')\n",
    "                #dataframeに追加\n",
    "                series = pd.Series( [\n",
    "                    index,          #文番号\n",
    "                    node.surface,   #表層\n",
    "                    feature[0],     #品詞1\n",
    "                    feature[1],     #品詞2     \n",
    "                    feature[2],     #品詞3\n",
    "                    feature[3],     #品詞4\n",
    "                    feature[6],     #原型\n",
    "                    node.posid      #品詞番号\n",
    "                ], index=df.columns)\n",
    "                df = df.append(series, ignore_index = True)\n",
    "    logging.debug(\"End : parse2df\")  \n",
    "    df2 = df[df[\"posID\"].isin([36,37,38,40,41,42,43,44,45,46,47,50,51,52,66,67,2,31,10,34])]\n",
    "    stop_words = [\"する\"]\n",
    "    df2 = df2[~df2[\"原型\"].isin(stop_words )]  # ~df.isin(list) で listに含まれないもの となる\n",
    "    return df2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.utils_any2vec:loading projection weights from ./entity_vector/entity_vector.model.bin\n",
      "INFO:gensim.models.utils_any2vec:loaded (1015474, 200) matrix from ./entity_vector/entity_vector.model.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('./entity_vector/entity_vector.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_seiriからcontentが空文字列のものを取り除いたものを読み込む\n",
    "df = pd.read_csv(\"./data/sample_seiri2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "doc_vec = []\n",
    "for i in range(len(df)):\n",
    "    if i%10 == 0:\n",
    "        print(i) #進捗確認用\n",
    "    title_and_content = df.loc[i][\"title\"] + df.loc[i][\"content\"]\n",
    "    words = parse2df(title_and_content)[\"原型\"]\n",
    "    tmp = [0]*200 #200次元で単語が表現されている\n",
    "    word_count = 0 #文章中の単語のうちモデルに登録されているものの数\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_count += 1\n",
    "            tmp = list(map(add, tmp, model[word]))\n",
    "    doc_vec.append(list(map(lambda x: x/word_count, tmp))) #平均を取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfにdoc_vecの項目を追加\n",
    "df2 = pd.DataFrame({\n",
    "    \"doc_vec\": doc_vec\n",
    "})\n",
    "df3 = pd.concat([df.loc[:,[\"title\",\"content\",\"tags\"]],df2], axis=1)\n",
    "\n",
    "#csvに出力\n",
    "df3.set_index(\"title\").to_csv(\"./data/sample_doc_vec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
